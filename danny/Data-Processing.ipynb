{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/postscndryunivsrvy2013dirinfo_cut.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ecbaa29eb72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read it in.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/postscndryunivsrvy2013dirinfo_cut.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/peps300.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Do some fixing and plumbing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/danny/miniconda3/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheetname, header, skiprows, skip_footer, index_col, names, parse_cols, parse_dates, date_parser, na_values, thousands, convert_float, has_index_names, converters, true_values, false_values, engine, squeeze, **kwds)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     return io._parse_excel(\n",
      "\u001b[0;32m/Users/danny/miniconda3/lib/python3.5/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m/Users/danny/miniconda3/lib/python3.5/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/postscndryunivsrvy2013dirinfo_cut.xlsx'"
     ]
    }
   ],
   "source": [
    "# # Read it in.\n",
    "# # post = pd.read_excel('../data/postscndryunivsrvy2013dirinfo_cut.xlsx')\n",
    "# peps = pd.read_excel('../data/peps300.xlsx')\n",
    "\n",
    "# # Do some fixing and plumbing\n",
    "# post['OPEID_CLEAN'] = [value[2:] for value in post['OPEID_CLEAN'].values]\n",
    "# peps['OPEID_CLEAN'] = [str(value) for value in peps['OPEID_CLEAN'].values]\n",
    "\n",
    "# # Do some more plumbing.\n",
    "# peps = peps.iloc[:, :20]\n",
    "\n",
    "# # Merge our datasets.\n",
    "# joined = pd.merge(post, peps, on='OPEID_CLEAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read it in.\n",
    "survey2011 = pd.read_csv('../data/hd2011.csv', encoding = \"ISO-8859-1\")\n",
    "survey2012 = pd.read_csv('../data/hd2012.csv', encoding = \"ISO-8859-1\")\n",
    "peps = pd.read_excel('../data/peps300.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do some fixing and plumbing\n",
    "#survey2011['OPEID_CLEAN'] = [value[2:] for value in post['OPEID'].values]  // I don't know why you guys are doing this, so I just choose not to use it for now\n",
    "#peps['OPEID_CLEAN'] = [str(value) for value in peps['OPEID_CLEAN'].values]\n",
    "\n",
    "# pepsG = the first 11 and columns which we used for both train and test\n",
    "pepsG = peps.iloc[:, :11]\n",
    "\n",
    "pepsG['OPEID'] = pepsG['OPEID_CLEAN']\n",
    "\n",
    "pepsG = pepsG.drop('OPEID_CLEAN', axis =1)\n",
    "\n",
    "pepsG['Ethnic Code'] = peps['Ethnic Code']\n",
    "\n",
    "pepsG['Average or\\nGreater than 30'] = peps['Average or\\nGreater than 30']\n",
    "\n",
    "\n",
    "#prepare the train set/2011\n",
    "peps11 = peps.iloc[:, 27:31]\n",
    "train = pd.merge(pepsG, peps11, left_index = True, right_index=True)\n",
    "\n",
    "# #prepare the test set/2012\n",
    "peps12 = peps.iloc[:, 22:26]\n",
    "test = pd.merge(pepsG, peps12,left_index = True, right_index=True)\n",
    "\n",
    "# # Merge our datasets.\n",
    "train = pd.merge(survey2011, train, on='OPEID')\n",
    "test = pd.merge(survey2012, test, on='OPEID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['UNITID', 'INSTNM', 'ADDR', 'CITY', 'STABBR', 'ZIP', 'FIPS',\n",
       "       'OBEREG', 'CHFNM', 'CHFTITLE', 'GENTELE', 'EIN', 'OPEID', 'OPEFLAG',\n",
       "       'WEBADDR', 'ADMINURL', 'FAIDURL', 'APPLURL', 'NPRICURL', 'SECTOR',\n",
       "       'ICLEVEL', 'CONTROL', 'HLOFFER', 'UGOFFER', 'GROFFER', 'HDEGOFR1',\n",
       "       'DEGGRANT', 'HBCU', 'HOSPITAL', 'MEDICAL', 'TRIBAL', 'LOCALE',\n",
       "       'OPENPUBL', 'ACT', 'NEWID', 'DEATHYR', 'CLOSEDAT', 'CYACTIVE',\n",
       "       'POSTSEC', 'PSEFLAG', 'PSET4FLG', 'RPTMTH', 'IALIAS', 'INSTCAT',\n",
       "       'CCBASIC', 'CCIPUG', 'CCIPGRAD', 'CCUGPROF', 'CCENRPRF', 'CCSIZSET',\n",
       "       'CARNEGIE', 'TENURSYS', 'LANDGRNT', 'INSTSIZE', 'CBSA', 'CBSATYPE',\n",
       "       'CSA', 'NECTA', 'F1SYSTYP', 'F1SYSNAM', 'FAXTELE', 'COUNTYCD',\n",
       "       'COUNTYNM', 'CNGDSTCD', 'LONGITUD', 'LATITUDE', 'Name', 'Address',\n",
       "       'City', 'State', 'State Desc', 'Zip Code', 'Zip Ext',\n",
       "       'Prog\\nLength', 'School\\nType', 'Ethnic Code',\n",
       "       'Average or\\nGreater than 30', 'Dual\\nNum 3', 'Dual\\nDenom 3',\n",
       "       'DRate 3', 'PRate 3'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ASK BRIAN:\n",
    "#  CBSA: population density measure\n",
    "#    CBSATYPE\n",
    "#    CSA\n",
    "#  Congressional district\n",
    "\n",
    "# Not including geo-location features like zip code, county.\n",
    "# TODO: revisit Latitude and Longitude for visualization.\n",
    "\n",
    "features = ['UNITID',\n",
    "            'CONTROL',\n",
    "            'GROFFER',\n",
    "            'HBCU',\n",
    "            'HLOFFER',\n",
    "            'HOSPITAL',\n",
    "            'ICLEVEL',\n",
    "            'INSTCAT',\n",
    "            'INSTSIZE',\n",
    "            'LANDGRNT', # Revisit\n",
    "            'LOCALE',\n",
    "            'PSEFLAG',\n",
    "            'SECTOR',\n",
    "            'Prog\\nLength',\n",
    "            'School\\nType',\n",
    "#             '\\nDenom 1',\n",
    "#             '\\nNum 1',\n",
    "            'UGOFFER',\n",
    "#             'DRate 3',\n",
    "            'Ethnic Code',\n",
    "            'Average or\\nGreater than 30']\n",
    "\n",
    "med = np.median(train['DRate 3'])\n",
    "\n",
    "# ax = plt.axes()\n",
    "# ax.set_title(\"KDE for Default Rate\")\n",
    "# kde = sns.kdeplot(joined['DRate 1'], bw = 0.4)\n",
    "# fig = kde.get_figure()\n",
    "# fig.savefig(\"kde_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danny/miniconda3/lib/python3.5/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Master dataset!!\n",
    "# joined = joined[features]\n",
    "\n",
    "train_features = features + ['DRate 3']\n",
    "test_features = features + ['DRate 2']\n",
    "\n",
    "train = train[train_features]\n",
    "test = test[test_features]\n",
    "\n",
    "train['DRate'] = train['DRate 3']\n",
    "test['DRate'] = test['DRate 2']\n",
    "train = train.drop('DRate 3', axis = 1)\n",
    "test = test.drop('DRate 2', axis = 1)\n",
    "\n",
    "# np.median(joined['DRate 1']) == 9.80000\n",
    "# med = np.median(train['DRate 3'])\n",
    "med = np.median(train['DRate'])\n",
    "\n",
    "# joined['DRate_Transformed'] = joined['DRate 1'] > med\n",
    "# joined['DRate_Transformed'] = joined['DRate_Transformed'].astype(int)\n",
    "\n",
    "train['DRate_Transformed'] = train['DRate'] > med\n",
    "train['DRate_Transformed'] = train['DRate_Transformed'].astype(int)\n",
    "\n",
    "# TODO eventually re-add this statement\n",
    "# joined = joined.drop('DRate 1', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our initial model works with THIS dataset.\n",
    "# joined.to_csv(\"../data/peps_to_post.csv\", index=False)\n",
    "train.to_csv(\"../data/joined_train.csv\", index=False)\n",
    "test.to_csv(\"../data/joined_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danny/miniconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "ipeds = pd.read_csv(\"../data/delta_public_00_12.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ipeds['is_2012'] = ipeds['academicyear'] == 2012\n",
    "# ipeds = ipeds[ipeds['is_2012']]\n",
    "\n",
    "ipeds['is_2011'] = ipeds['academicyear'] == 2011\n",
    "ipeds2011 = ipeds[ipeds['is_2011']]\n",
    "\n",
    "ipeds['is_2012'] = ipeds['academicyear'] == 2012\n",
    "ipeds2012 = ipeds[ipeds['is_2012']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# REMOVE instname, unitid WHEN DONE\n",
    "\n",
    "ipeds_features =\"\"\"\n",
    "instname\n",
    "unitid\n",
    "sector\n",
    "iclevel\n",
    "control\n",
    "census_region\n",
    "hbcu\n",
    "hsi\n",
    "cpi_index\n",
    "hepi_index\n",
    "has_fte\n",
    "nettuition01\n",
    "net_student_tuition\n",
    "endowment03\n",
    "priv_invest_endow\n",
    "total03_revenue\n",
    "restricted_revenue\n",
    "grant01\n",
    "grant02\n",
    "grant07\n",
    "tuition_discount\n",
    "institutional_grant_aid_share\n",
    "fed_grant_pct\n",
    "inst_grant_pct\n",
    "loan_pct\n",
    "loan_avg_amount\n",
    "inst_grant_avg_amount\n",
    "tuitionfee02_tf\n",
    "studserv01\n",
    "acadsupp01\n",
    "sticker_price_share\n",
    "nettuition_share\n",
    "totaldegrees_100fte\n",
    "certificates_awards_100fte\n",
    "totalcompletions_100fte\n",
    "ft_first_time_first_yr_deg_seek\n",
    "total_full_time\n",
    "total_enrollment_asian_tot\n",
    "total_enrollment_black_tot\n",
    "total_enrollment_hisp_tot\n",
    "total_enrollment_white_tot\n",
    "ft_faculty_per_100fte\n",
    "pt_faculty_per_100fte\n",
    "salarytotal\"\"\".split()\n",
    "\n",
    "# Isolate wanted columns, map column headers to uppercase for join.\n",
    "# ipeds = ipeds[ipeds_features]\n",
    "# ipeds.columns = map(str.upper, ipeds.columns)\n",
    "ipeds2011 = ipeds2011[ipeds_features]\n",
    "ipeds2012 = ipeds2012[ipeds_features]\n",
    "ipeds2011.columns = map(str.upper, ipeds2011.columns)\n",
    "ipeds2012.columns = map(str.upper, ipeds2012.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge :joined and :ipeds on the UNITID column.\n",
    "# master = pd.merge(joined, ipeds, on=\"UNITID\")\n",
    "\n",
    "train = pd.merge(train, ipeds2011, on=\"UNITID\")\n",
    "test = pd.merge(test, ipeds2012, on=\"UNITID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49    American Institute of Technology\n",
       "Name: INSTNAME, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# master[master['UNITID'] == 103811].INSTNAME\n",
    "train[train['UNITID'] == 103811].INSTNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set our institution's UNITID to be its index, drop\n",
    "# the corresponding UNITID column.\n",
    "# master.index = master['UNITID']\n",
    "# master = master.drop(\"UNITID\", axis = 1)\n",
    "\n",
    "train.index = train['UNITID']\n",
    "train = train.drop(\"UNITID\", axis = 1)\n",
    "\n",
    "test.index = test['UNITID']\n",
    "test = test.drop(\"UNITID\", axis = 1)\n",
    "\n",
    "# Leave this here please :)\n",
    "# master_copy = master\n",
    "# master_copy.index = master_copy['INSTNAME']\n",
    "# master_copy = master_copy.drop(\"INSTNAME\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Master dataset! Woo.\n",
    "# master.to_csv(\"../data/master.csv\")\n",
    "train.to_csv(\"../data/master_train.csv\")\n",
    "\n",
    "# Design school with NaN DRate\n",
    "test = test.drop([134680,456296])\n",
    "\n",
    "test.to_csv(\"../data/master_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.isnull(master).any()\n",
    "nans = pd.isnull(master)\n",
    "nans_copy = pd.isnull(master_copy)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "university_nans = Counter()\n",
    "column_count_nans = Counter()\n",
    "\n",
    "for index, row in zip(nans_copy.index, nans_copy.values):\n",
    "    row_data = dict(zip(nans_copy.columns.values, row))\n",
    "\n",
    "    for column, value in row_data.items():\n",
    "        if value:\n",
    "            university_nans[index] += 1\n",
    "            column_count_nans[column] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A good number of our institutions have between 0 and 10 NaN values.\n",
    "university_nan_dist = pd.DataFrame.from_dict(university_nans, orient='index').reset_index()\n",
    "university_nan_dist = university_nan_dist.rename(columns={'index':'event', 0:'count'})\n",
    "plt.hist(university_nan_dist['count'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_nan_dist = pd.DataFrame.from_dict(column_count_nans, orient='index').reset_index()\n",
    "column_nan_dist = column_nan_dist.rename(columns={'index':'event', 0:'count'})\n",
    "column_nan_dist['count'] = column_nan_dist['count'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot new features against their NaN count\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = range(len(column_nan_dist['event']))\n",
    "plt.xticks(x, column_nan_dist['event'])\n",
    "ax.set_xticklabels(column_nan_dist['event'], rotation = 90)\n",
    "plt.plot(x, column_nan_dist['count'], \"g\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# master = master.drop('ENDOWMENT03', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Isolate rows where master contains no missing values\n",
    "not_nans = master.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get percent of valid rows in master\n",
    "valid_percent_of_master = 1 - (master.shape[0] - not_nans.shape[0]) / master.shape[0]\n",
    "valid_percent_of_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot new default rate based on just the valid rows after dropping ENDOWMENT03\n",
    "ax.set_title(\"KDE for Default Rate\")\n",
    "kde = sns.kdeplot(not_nans['DRate 1'], bw = 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the distribution median for this non-null dataset.\n",
    "# Note that the median declined from 9.8 to 9.2\n",
    "np.median(not_nans['DRate 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
